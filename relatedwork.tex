\chapter{Related Work}

There has been work on using user ratings prediction for stylistic surface realisation (\cite{dethlefs2014cluster}). The study used ratings by users for generated texts along three axes of style, colloquialism, naturalness and politeness. The study then clustered users according to their ratings, and used stylistic predictions from this cluster towards the surface realization of new text. The three axes chosen were fairly random, and may not have been completely independent. However, the concept of rating documents according to the stylistic characteristics of the text and using this stylistic information to rate the newly generated text is something that can be explored further. 

Brooke et al., 2012 \cite{brooke2012building} describe the process of building a formality lexicon by analyzing the stylistics of text. They calculate formality scores for words and sentences by training a model on a large corpus based on the appearance of words in specific document. Their model represents words as vectors and the formal and informal seeds appear in opposite halves of the graphs, suggesting that we can use these seeds to determine if an article is formal or informal. Brooke et al., 2013 \cite{brooke2013multi} used an LDA based model using a similar idea of seed words for getting stylistic rankings for documents. The documents were ranked for styles such as literary, colloquial, subjective, concrete, and so on. 

There have also been studies specific to Twitter data, for classifying and summarizing text, intents, etc. Ghosh et al., 2011 \cite{ghosh2011entropy}, classified the retweeting activity of users based on entropy. The study considered the occurrence of the same URL in a different tweet as a ‘retweet’, and was able to separate the tweets as automatic or robotic retweeting, campaigns, news, blogs and so on. The study shows some interesting trends of retweeting activity for each of these cases. In another study, Chen et al., 2012, were able to extract sentiment expressions based on their corpus of tweets, and were able to extract both formal and slang sentiment bearing words. 

There has also been an attempt at generating tweets, texts of 140 characters using different text summarization techniques (Loret et al., 2013 \cite{lloret2013towards}). Summarization systems were used to summarize texts to sentences and then were compared against each other, evaluated using the ROUGE metric for evaluation. The ROUGE-1, ROUGE-2 and ROUGE-L metrics were used and the tweets were compared against an ideal summary. ROUGE is better when used with multiple reference texts and is not meant to be used at the sentence level. Thus the evaluation is done using the unigram, bigram and longest common subsequence matching techniques used in ROUGE-1,2 and L. None of these techniques evaluate the fluency of the text, which is generally not expected from extractive summarization. 

This problem can be addressed using NLG techniques for tweet or a small summary generation.
