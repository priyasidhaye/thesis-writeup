\chapter {Experiments}

\section {Subjectivity and Formality }

After tagging a sample set of articles, the natural next task was to determine if the articles could be tagged automatically based on characteristics of the text. To achieve this, the degree of subjectivity and formality of the text was calculated with the help of some other studies. The subjectivity lexicon (Wilson et al., 2005 \cite{wilson2005recognizing}) was built using data for subjectivity analysis for a given text. The subjectivity lexicon consists of words that might indicate an opinion being expressed in a given text. Similarly, the formality lexicon gives was generated by Brooke et al. 2013 \cite{brooke2013multi} and can be used to measure formality of a given text. The lexicon consists of words and phrases and the degree of formality for their occurrence. Thus, more formal words marked on a positive scale and informal words like those occurring in colloquial language are marked on a negative scale. Using the formality and subjectivity lexicons, the degree of subjectivity and formality of each individual article was calculated. 

The degree of subjectivity returned a count per of the number of words present in the article that suggested an opinion per article. This number was normalized with the length of the article, and the degree of subjectivity was calculated per 10 words of an article. For this result, only the strong subjective entries in the lexicon were used to better differentiate between subjective and non-subjective articles.

The formality lexicon gave positive weights for formal expressions and negative for informal expressions. After calculating the formality weights for all articles, it was observed that they all had a total negative normalized weight, meaning a lot more informal expressions were getting matched. Hence, we used just the formal word occurrences for calculating the weight. Thus, above a certain cut-off weight, the article could be considered formal, else would be considered informal.

All the weights from both lexicons were averaged out over the articles relating to a single search term(or hashtag), and then ranked accordingly. The ranking showed that for subjectivity ranking over hashtags, films and music related hashtags are at the top, which would be the natural intuition given the nature of the topics. On the other hand, in the formality ranking, the hashtags relating to political issues had the highest formality ranking, while the hashtags for film titles, pop culture are all at the bottom. This also correlates with intuition about the topics. As a sanity check, we also looked at articles at the extreme points of the both the graphs. The texts of these articles suggested that they were consistent with the numbers.

Correlation between the rankings of hashtags given by both these experiments was calculated, and the Kendallâ€™s tau for this was 0.09 with a p-value of 0.34. The low correlation suggests that these two ways of evaluating subjectivity and formality are independent. The p-value suggests that there is not enough evidence to prove a correlation between subjectivity and formality of an article.

\section{Correlating descriptive/non-descriptive with formal vs. informal for automatic tagging}

To check if the descriptive vs non-descriptive tags correlated when tagged using the formality lexicon. If the document contained no formal words from the lexicon, it was tagged as non-traditional, else, it was tagged as traditional. The sample set of articles was tagged using this method, and after comparing them with the human tags, 42 out of the 62 tags matched, which gave a match percentage of 67%.

\section{Position of tweet text in article experiments}
\subsection {Total match with text in article}

We calculated the position of tweet text as a whole in the text. To compare the text, we removed the hashtags, references (@) and urls from the tweets. After this, we did direct substring comparison of the tweet in the text. 

Out of the 6144 instances where a tweet text was checked against the text in the article, a complete match was found around 70 times. 30 times out of these, the tweet text had been matched against the title of the article extracted into the text. The rest of the results are significant, since the text of the tweet appears exactly as is inside the text of the article. The user who wrote the tweet for these articles went through the article text, and the sentence that either seemed to be the most conclusive contribution of the article, or expressed the opinion of the user were extracted to be tweeted. 

We also checked to see if the tweet text matched a lot with the article titles, and this was found not to be the case. (*Needs to be verified)

\subsection{Percentage match}

Next, we did a percentage match with the text of the article after removing the stop words from both the tweet and the text. The results we got seem to suggest that a lot of significant words in the tweet are in fact present in the article. The minimum percentage match obtained was 60%.

\subsection{Percentage matching inside a window in the article text}

The next analysis was to check for a significant word matching inside a two or three sentence window inside the article text. We used a three sentence long window using the sentence boundary information obtained during preprocessing. After the text of the window was extracted, we performed a similar analysis as the last one, except on a smaller text. Next, the matching percentages from all such windows in the articles were compared and the maximum out of these was considered for the highest match percentage and match position for the final results. The final results are being verified, including the result for where the tweet text mostly comes from is random.

\subsection{Least Common Subsequence match inside a window for the text}

The percentage matched have mostly been a bag-of-words approach. The next step would be to look for phrases in the tweet coming directly from the text.
